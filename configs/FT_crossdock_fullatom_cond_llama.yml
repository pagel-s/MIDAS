run_name: 'real_all_embed_tinyllama'
logdir: '/mnt/STORAGE3/sebastian2/data_hackathon/logs'
wandb_params:
  mode: 'online'  # disabled, offline, online
  entity: '2838102p'
  group: 'crossdock_lang_llama'
dataset: 'crossdock'
datadir: '/mnt/STORAGE3/sebastian2/data_hackathon/data/crossdocked_pocket10_proc_ca_only'
enable_progress_bar: True
num_sanity_val_steps: 0

mode: 'pocket_conditioning'  # joint, pocket_conditioning
pocket_representation: 'CA'  # CA, full-atom
batch_size: 64
lr: 1.0e-3
noise_factor: 1.0
n_epochs: 200
num_workers: 0
gpus: 1
clip_grad: True
augment_rotation: False
augment_noise: 0
accumulate_grad_batches: 1

auxiliary_loss: False
loss_params:
  max_weight: 0.001
  schedule: 'linear'
  clamp_lj: 3.0

egnn_params:
  device: 'cuda'
  edge_cutoff_ligand: null
  edge_cutoff_pocket: 5.0
  edge_cutoff_interaction: 5.0
  reflection_equivariant: False
  joint_nf: 128
  hidden_nf: 256
  n_layers: 6
  attention: True
  tanh: True
  norm_constant: 1
  inv_sublayers: 1
  sin_embedding: False
  aggregation_method: 'sum'
  normalization_factor: 100 #1  # used if aggregation_method='sum'

diffusion_params:
  diffusion_steps: 500
  diffusion_noise_schedule: 'polynomial_2'  # learned, cosine
  diffusion_noise_precision: 5.0e-4
  diffusion_loss_type: 'l2'  # vlb, l2
  normalize_factors: [1, 4]  #[10, 4]  # [x, h]

eval_epochs: 50
visualize_sample_epoch: 50
visualize_chain_epoch: 50
eval_params:
  n_eval_samples: 100
  eval_batch_size: 100
  smiles_file: '/mnt/STORAGE3/sebastian2/data_hackathon/data/crossdocked_pocket10_proc_ca_only/train_smiles.npy'
  n_visualize_samples: 5
  keep_frames: 100

text_model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
# text_csv: "./data/training_descriptions_mixed.csv"
text_embeddings_path: "/mnt/STORAGE3/sebastian2/data_hackathon/data/real_all_embed_tinyllama.npz"
